# handler.py - WORKING VERSION
import runpod
from llama_cpp import Llama
import os
import sys
import time

print("=" * 60)
print("ğŸš€ LLaMA 3D Gift Ideas Generator")
print("=" * 60)

# Debug info
print(f"\nğŸ Python version: {sys.version}")
print(f"ğŸ“ Working directory: {os.getcwd()}")

# Check for network volume
print("\nğŸ” Checking for model file...")
possible_paths = [
    "/workspace/Llama-3.2-3B-Instruct-IQ3_M.gguf",
    "/model/Llama-3.2-3B-Instruct-IQ3_M.gguf",
    "/volume/Llama-3.2-3B-Instruct-IQ3_M.gguf",
]

model_path = None
for path in possible_paths:
    if os.path.exists(path):
        model_path = path
        print(f"âœ… Model found at: {path}")
        size = os.path.getsize(path) / (1024**3)
        print(f"   Size: {size:.2f} GB")
        break

if not model_path:
    print("âŒ Model not found in any location!")
    print("\nğŸ’¡ Check endpoint configuration:")
    print("1. Go to Serverless â†’ llama-3d-gifts â†’ Manage")
    print("2. Verify Network Volume is attached")
    print("3. Check Mount Path matches one of the above")
    print("\nğŸ“‹ Root directory (/):")
    for item in sorted(os.listdir('/')):
        if os.path.isdir(os.path.join('/', item)):
            print(f"  ğŸ“ {item}/")
    sys.exit(1)

# Load model
print(f"\nğŸ”§ Loading model from: {model_path}")
print("This may take 30-60 seconds on CPU...")

try:
    llm = Llama(
        model_path=model_path,
        n_ctx=1024,      # Context size
        n_threads=4,     # CPU threads
        n_gpu_layers=0,  # CPU only
        verbose=True     # Show loading progress
    )
    print("âœ… Model loaded successfully!")
except Exception as e:
    print(f"âŒ Model loading failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

def handler(job):
    """Main handler function - REQUIRED for RunPod"""
    print(f"\nğŸ¯ Received job: {job.get('id', 'unknown')}")
    
    try:
        input_data = job["input"]
        fun_fact = input_data.get("fun_fact", "").strip()
        
        if not fun_fact:
            return {"error": "Please provide a 'fun_fact'"}
        
        print(f"ğŸ“ Processing: {fun_fact}")
        
        prompt = f"""Generate 2-3 creative 3D printable gift ideas for someone who: {fun_fact}

For each idea provide:
â€¢ Name
â€¢ Brief description  
â€¢ Why it's suitable for 3D printing

Keep responses practical and concise."""

        print("ğŸ¤– Generating ideas...")
        start_time = time.time()
        
        response = llm(
            prompt,
            max_tokens=300,
            temperature=0.7,
            top_p=0.9,
            echo=False
        )
        
        generation_time = time.time() - start_time
        print(f"â±ï¸ Generation took: {generation_time:.2f} seconds")
        
        result = response['choices'][0]['text'].strip()
        
        return {
            "status": "success",
            "ideas": result,
            "input": fun_fact,
            "generation_time": f"{generation_time:.2f}s",
            "model": "Llama-3.2-3B-Instruct"
        }
        
    except Exception as e:
        print(f"âŒ Handler error: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}

# CRITICAL: Start the RunPod server
print("\nğŸ Starting RunPod serverless handler...")
print("Server is ready to accept requests!")
runpod.serverless.start({"handler": handler})